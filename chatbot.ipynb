{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMW3GEbdhny6rovlp1Q9zPA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2805b84de7da4768aec4a456a972c32a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f7662afd58a4a66a038576161239b01",
              "IPY_MODEL_ed7b7132b8034faab2c4ca0f1457817e",
              "IPY_MODEL_bd43c0b7380848ca895fe3549f5cd026"
            ],
            "layout": "IPY_MODEL_72f068ea39f34f029befd61e6eed8a01"
          }
        },
        "8f7662afd58a4a66a038576161239b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80bce61590774183a958d9bef75e4570",
            "placeholder": "​",
            "style": "IPY_MODEL_db465a1d947c4be8b644f42fbf2bed0d",
            "value": "Epoch 1: 100%"
          }
        },
        "ed7b7132b8034faab2c4ca0f1457817e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70afa97ef56343bd8bf626cada248f33",
            "max": 135,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72d474e6c59c4c3eb74472b6bade534a",
            "value": 135
          }
        },
        "bd43c0b7380848ca895fe3549f5cd026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c04c72453dae4ac0a1dc620a302bba66",
            "placeholder": "​",
            "style": "IPY_MODEL_0c23e4fc5b1f4f1a9d53065d89a70126",
            "value": " 135/135 [00:43&lt;00:00,  2.88it/s]"
          }
        },
        "72f068ea39f34f029befd61e6eed8a01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80bce61590774183a958d9bef75e4570": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db465a1d947c4be8b644f42fbf2bed0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70afa97ef56343bd8bf626cada248f33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72d474e6c59c4c3eb74472b6bade534a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c04c72453dae4ac0a1dc620a302bba66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c23e4fc5b1f4f1a9d53065d89a70126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3740b930a986432eb2bf268b3a1802b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9fa09fad529e4e7288dd6da39d44b251",
              "IPY_MODEL_4a90c0b84d1a45fe8bf0b0b4a549cd4b",
              "IPY_MODEL_0ae40bd0934742de80d3d0463f9323a6"
            ],
            "layout": "IPY_MODEL_efb2eaea978a4163a0031a6ad22aff2d"
          }
        },
        "9fa09fad529e4e7288dd6da39d44b251": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5cd4d4e529640ddad1e8d4e03e1e8d0",
            "placeholder": "​",
            "style": "IPY_MODEL_6cf01f160d9a4b488d6ae523286b9a80",
            "value": "Epoch 2: 100%"
          }
        },
        "4a90c0b84d1a45fe8bf0b0b4a549cd4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d9ff4ab32ad455b8c1da6642b6766e4",
            "max": 135,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96252479d7c94deeb230fec25f93068f",
            "value": 135
          }
        },
        "0ae40bd0934742de80d3d0463f9323a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d732757ace084912a9a7eda78b729d7c",
            "placeholder": "​",
            "style": "IPY_MODEL_2774164fcca8488cba8354a5383e09b8",
            "value": " 135/135 [00:42&lt;00:00,  3.52it/s]"
          }
        },
        "efb2eaea978a4163a0031a6ad22aff2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5cd4d4e529640ddad1e8d4e03e1e8d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cf01f160d9a4b488d6ae523286b9a80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d9ff4ab32ad455b8c1da6642b6766e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96252479d7c94deeb230fec25f93068f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d732757ace084912a9a7eda78b729d7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2774164fcca8488cba8354a5383e09b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e3e25d0a25148cebf4c4135a2c29810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da69ef6419c549aa908fcd5b5a161cd5",
              "IPY_MODEL_a6ed2c195ae84ff0b4c9ffe0884eed20",
              "IPY_MODEL_11d703e72c0d44968bfad1f5e2b51506"
            ],
            "layout": "IPY_MODEL_5333241cf6dc496da8104f59eff4cad0"
          }
        },
        "da69ef6419c549aa908fcd5b5a161cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e5c5124f0854c26aaf0fd53a6c7149b",
            "placeholder": "​",
            "style": "IPY_MODEL_07f0ace4af3e49089525587004830285",
            "value": "Epoch 3: 100%"
          }
        },
        "a6ed2c195ae84ff0b4c9ffe0884eed20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc93d97276fa4727a664bb61371e1db8",
            "max": 135,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6bcffff8f62248359d6660270054c575",
            "value": 135
          }
        },
        "11d703e72c0d44968bfad1f5e2b51506": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6336fb10bf4f4e2690f179db26ef9efa",
            "placeholder": "​",
            "style": "IPY_MODEL_afc581deb5404f968f42e2278a2b1175",
            "value": " 135/135 [00:44&lt;00:00,  3.41it/s]"
          }
        },
        "5333241cf6dc496da8104f59eff4cad0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e5c5124f0854c26aaf0fd53a6c7149b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07f0ace4af3e49089525587004830285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc93d97276fa4727a664bb61371e1db8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bcffff8f62248359d6660270054c575": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6336fb10bf4f4e2690f179db26ef9efa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afc581deb5404f968f42e2278a2b1175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thedavidemmanuel/chatbot-with-transformers/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "latbp48uQQxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Setup and Imports\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    GPT2Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import json\n",
        "from google.colab import drive\n",
        "import gc\n",
        "from torch.cuda import empty_cache\n",
        "import torch.nn as nn\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Memory optimization settings\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define paths\n",
        "DATA_PATH = \"/content/chatbot.csv\"\n",
        "SAVE_DIR = \"/content/drive/My Drive/chatbot\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "PkW_lo2dQRNz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0278d6e8-b31a-4b89-c003-ccac0cce0779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Helper Classes\n",
        "class ConversationHistory:\n",
        "    def __init__(self, max_history=5):\n",
        "        self.history = []\n",
        "        self.max_history = max_history\n",
        "\n",
        "    def add_exchange(self, category, intent, question, response, confidence):\n",
        "        self.history.append({\n",
        "            'category': category,\n",
        "            'intent': intent,\n",
        "            'question': question,\n",
        "            'response': response,\n",
        "            'confidence': confidence,\n",
        "            'timestamp': pd.Timestamp.now()\n",
        "        })\n",
        "        if len(self.history) > self.max_history:\n",
        "            self.history.pop(0)\n",
        "\n",
        "    def get_context(self):\n",
        "        return self.history\n",
        "\n",
        "    def clear(self):\n",
        "        self.history = []\n",
        "\n",
        "class ChatbotDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data.iloc[idx]\n",
        "        question = str(item['user_input'])\n",
        "        answer = str(item['bot_response'])\n",
        "        category = str(item['category'])\n",
        "        intent = str(item['intent'])\n",
        "\n",
        "        input_text = (\n",
        "            f\"<|category|>{category}<|/category|>\"\n",
        "            f\"<|intent|>{intent}<|/intent|>\"\n",
        "            f\"<|question|>{question}<|/question|>\"\n",
        "            f\"<|response|>{answer}<|/response|>\"\n",
        "        )\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            input_text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': encoding['input_ids'].squeeze(0)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "z3xJTvd1T2R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Memory-Optimized Chatbot\n",
        "class MemoryOptimizedChatBot:\n",
        "    def __init__(self, model_name='gpt2', batch_size=4):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.batch_size = batch_size\n",
        "        self.conversation_history = ConversationHistory()\n",
        "        self.response_cache = {}\n",
        "\n",
        "        # Add special tokens\n",
        "        special_tokens = {\n",
        "            'pad_token': '<|pad|>',\n",
        "            'additional_special_tokens': [\n",
        "                '<|category|>', '<|/category|>',\n",
        "                '<|intent|>', '<|/intent|>',\n",
        "                '<|question|>', '<|/question|>',\n",
        "                '<|response|>', '<|/response|>',\n",
        "                '<|context|>', '<|/context|>'\n",
        "            ]\n",
        "        }\n",
        "        self.tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "        # Initialize model with memory optimizations\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "        self.model.config.pad_token_id = self.tokenizer.pad_token_id\n",
        "\n",
        "        # Enable memory optimizations\n",
        "        self.model.gradient_checkpointing_enable()\n",
        "        self.model.config.n_ctx = 512\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Initialize mixed precision training\n",
        "        self.scaler = GradScaler()\n",
        "\n",
        "        logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "    def train(self, train_dataset, val_dataset, epochs=3):\n",
        "        try:\n",
        "            # Clear memory before training\n",
        "            gc.collect()\n",
        "            empty_cache()\n",
        "\n",
        "            train_loader = DataLoader(\n",
        "                train_dataset,\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=True,\n",
        "                pin_memory=True,\n",
        "                num_workers=2\n",
        "            )\n",
        "\n",
        "            val_loader = DataLoader(\n",
        "                val_dataset,\n",
        "                batch_size=self.batch_size,\n",
        "                pin_memory=True\n",
        "            )\n",
        "\n",
        "            # Optimizer and scheduler\n",
        "            optimizer = torch.optim.AdamW(\n",
        "                self.model.parameters(),\n",
        "                lr=2e-5,\n",
        "                weight_decay=0.01,\n",
        "                eps=1e-8\n",
        "            )\n",
        "\n",
        "            accumulation_steps = 4\n",
        "            scheduler = get_linear_schedule_with_warmup(\n",
        "                optimizer,\n",
        "                num_warmup_steps=len(train_loader) // 10,\n",
        "                num_training_steps=len(train_loader) * epochs\n",
        "            )\n",
        "\n",
        "            best_val_loss = float('inf')\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                logger.info(f\"Starting epoch {epoch + 1}/{epochs}\")\n",
        "                self.model.train()\n",
        "                total_train_loss = 0\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                for batch_idx, batch in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}')):\n",
        "                    try:\n",
        "                        # Use mixed precision training\n",
        "                        with autocast():\n",
        "                            inputs = {\n",
        "                                'input_ids': batch['input_ids'].to(self.device, non_blocking=True),\n",
        "                                'attention_mask': batch['attention_mask'].to(self.device, non_blocking=True),\n",
        "                                'labels': batch['labels'].to(self.device, non_blocking=True)\n",
        "                            }\n",
        "\n",
        "                            outputs = self.model(**inputs)\n",
        "                            loss = outputs.loss / accumulation_steps\n",
        "\n",
        "                        # Scale loss and backward pass\n",
        "                        self.scaler.scale(loss).backward()\n",
        "                        total_train_loss += loss.item() * accumulation_steps\n",
        "\n",
        "                        if (batch_idx + 1) % accumulation_steps == 0:\n",
        "                            self.scaler.unscale_(optimizer)\n",
        "                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                            self.scaler.step(optimizer)\n",
        "                            self.scaler.update()\n",
        "                            scheduler.step()\n",
        "                            optimizer.zero_grad()\n",
        "\n",
        "                            # Clear memory\n",
        "                            del outputs\n",
        "                            gc.collect()\n",
        "                            if torch.cuda.is_available():\n",
        "                                empty_cache()\n",
        "\n",
        "                    except RuntimeError as e:\n",
        "                        if \"out of memory\" in str(e):\n",
        "                            if torch.cuda.is_available():\n",
        "                                empty_cache()\n",
        "                            logger.warning(f\"OOM on batch {batch_idx}. Skipping...\")\n",
        "                            if hasattr(optimizer, 'zero_grad'):\n",
        "                                optimizer.zero_grad()\n",
        "                            continue\n",
        "                        raise e\n",
        "\n",
        "                avg_train_loss = total_train_loss / len(train_loader)\n",
        "                logger.info(f\"Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "                # Validation\n",
        "                val_loss = self.evaluate(val_loader)\n",
        "                logger.info(f\"Validation loss: {val_loss:.4f}\")\n",
        "\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    self.save_model(epoch + 1, val_loss)\n",
        "\n",
        "                # Clear memory after each epoch\n",
        "                gc.collect()\n",
        "                if torch.cuda.is_available():\n",
        "                    empty_cache()\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Training error: {str(e)}\")\n",
        "            raise e\n",
        "\n",
        "    def save_model(self, epoch, loss):\n",
        "        checkpoint_dir = os.path.join(SAVE_DIR, 'checkpoints')\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f'model_epoch_{epoch}_loss_{loss:.4f}.pt')\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'tokenizer_state': self.tokenizer.save_pretrained(checkpoint_dir),\n",
        "            'loss': loss\n",
        "        }, checkpoint_path)\n",
        "        logger.info(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "\n",
        "    def evaluate(self, val_loader):\n",
        "        \"\"\"Memory-efficient evaluation\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                try:\n",
        "                    inputs = {\n",
        "                        'input_ids': batch['input_ids'].to(self.device, non_blocking=True),\n",
        "                        'attention_mask': batch['attention_mask'].to(self.device, non_blocking=True),\n",
        "                        'labels': batch['labels'].to(self.device, non_blocking=True)\n",
        "                    }\n",
        "\n",
        "                    outputs = self.model(**inputs)\n",
        "                    total_loss += outputs.loss.item()\n",
        "\n",
        "                    del outputs\n",
        "                    if torch.cuda.is_available():\n",
        "                        empty_cache()\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    if \"out of memory\" in str(e):\n",
        "                        if torch.cuda.is_available():\n",
        "                            empty_cache()\n",
        "                        logger.warning(\"OOM during evaluation. Skipping batch...\")\n",
        "                        continue\n",
        "                    raise e\n",
        "\n",
        "        return total_loss / len(val_loader)\n",
        "\n",
        "    def clean_response(self, response):\n",
        "        \"\"\"Clean the generated response by removing special tokens and extracting content\"\"\"\n",
        "        try:\n",
        "            start_token = \"<|response|>\"\n",
        "            end_token = \"</|response|>\"\n",
        "\n",
        "            start_idx = response.find(start_token) + len(start_token)\n",
        "            end_idx = response.find(end_token)\n",
        "\n",
        "            if start_idx == -1 or end_idx == -1:\n",
        "                return response.strip()\n",
        "\n",
        "            cleaned = response[start_idx:end_idx].strip()\n",
        "            return cleaned\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error cleaning response: {str(e)}\")\n",
        "            return response.strip()\n",
        "\n",
        "    def generate_response(self, category, intent, question, max_length=150):\n",
        "        \"\"\"Generate a response with error handling\"\"\"\n",
        "        try:\n",
        "            cache_key = f\"{category}_{intent}_{question}\"\n",
        "            if cache_key in self.response_cache:\n",
        "                return self.response_cache[cache_key], 1.0\n",
        "\n",
        "            self.model.eval()\n",
        "            input_text = (\n",
        "                f\"<|category|>{category}<|/category|>\"\n",
        "                f\"<|intent|>{intent}<|/intent|>\"\n",
        "                f\"<|question|>{question}<|/question|>\"\n",
        "                f\"<|response|>\"\n",
        "            )\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                input_text,\n",
        "                return_tensors='pt',\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                padding=True\n",
        "            ).to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output_sequences = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_length=max_length,\n",
        "                    min_length=20,\n",
        "                    num_return_sequences=1,\n",
        "                    no_repeat_ngram_size=3,\n",
        "                    do_sample=True,\n",
        "                    top_k=50,\n",
        "                    top_p=0.92,\n",
        "                    temperature=0.7,\n",
        "                    repetition_penalty=1.2,\n",
        "                    length_penalty=1.0,\n",
        "                    early_stopping=True,\n",
        "                    pad_token_id=self.tokenizer.pad_token_id,\n",
        "                    eos_token_id=self.tokenizer.eos_token_id,\n",
        "                )\n",
        "\n",
        "            response = self.tokenizer.decode(output_sequences[0], skip_special_tokens=False)\n",
        "            cleaned_response = self.clean_response(response)\n",
        "\n",
        "            # Cache the response\n",
        "            self.response_cache[cache_key] = cleaned_response\n",
        "            return cleaned_response\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating response: {str(e)}\")\n",
        "            return \"I apologize, but I encountered an error generating a response. Please try again.\""
      ],
      "metadata": {
        "id": "dpAtpzgJT_MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Training Script\n",
        "def train_chatbot():\n",
        "    # Clear memory\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        empty_cache()\n",
        "\n",
        "    # Load data\n",
        "    logger.info(\"Loading dataset...\")\n",
        "    data = pd.read_csv(DATA_PATH)\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"\\nDataset Statistics:\")\n",
        "    print(f\"Total conversations: {len(data)}\")\n",
        "    print(\"\\nCategory distribution:\")\n",
        "    print(data['category'].value_counts())\n",
        "    print(\"\\nIntent distribution:\")\n",
        "    print(data['intent'].value_counts())\n",
        "\n",
        "    # Save metadata\n",
        "    metadata = {\n",
        "        \"total_conversations\": len(data),\n",
        "        \"categories\": data['category'].unique().tolist(),\n",
        "        \"intents\": data['intent'].unique().tolist(),\n",
        "        \"model_config\": {\n",
        "            \"model_type\": \"gpt2\",\n",
        "            \"max_length\": 512,\n",
        "            \"batch_size\": 4,\n",
        "            \"accumulation_steps\": 4\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(SAVE_DIR, \"metadata.json\"), 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "\n",
        "    # Split data\n",
        "    train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
        "\n",
        "    # Initialize bot and datasets\n",
        "    bot = MemoryOptimizedChatBot()\n",
        "    train_dataset = ChatbotDataset(train_data, bot.tokenizer)\n",
        "    val_dataset = ChatbotDataset(val_data, bot.tokenizer)\n",
        "\n",
        "    # Train\n",
        "    logger.info(\"Starting training...\")\n",
        "    bot.train(train_dataset, val_dataset)\n",
        "\n",
        "    return bot, metadata"
      ],
      "metadata": {
        "id": "KsU5bD4QUGnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Main Execution\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Train model\n",
        "        bot, metadata = train_chatbot()\n",
        "\n",
        "        # Save model for Streamlit\n",
        "        final_model_path = os.path.join(SAVE_DIR, 'final_model')\n",
        "        bot.model.save_pretrained(final_model_path)\n",
        "        bot.tokenizer.save_pretrained(final_model_path)\n",
        "\n",
        "        # Create model info\n",
        "        model_info = {\n",
        "            \"model_path\": final_model_path,\n",
        "            \"categories\": metadata['categories'],\n",
        "            \"intents\": metadata['intents'],\n",
        "            \"total_samples\": metadata['total_conversations'],\n",
        "            \"model_config\": metadata['model_config']\n",
        "        }\n",
        "\n",
        "        with open(os.path.join(SAVE_DIR, 'model_info.json'), 'w') as f:\n",
        "            json.dump(model_info, f, indent=2)\n",
        "\n",
        "        # Zip the model files for easy download\n",
        "        !zip -r /content/chatbot_model.zip {SAVE_DIR}\n",
        "\n",
        "        print(\"\\nTraining complete! Model saved and ready for Streamlit deployment.\")\n",
        "        print(\"Download the chatbot_model.zip file and extract it to your Streamlit project directory.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main execution: {str(e)}\")\n",
        "        raise e"
      ],
      "metadata": {
        "id": "vlfF2sODULbt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706,
          "referenced_widgets": [
            "2805b84de7da4768aec4a456a972c32a",
            "8f7662afd58a4a66a038576161239b01",
            "ed7b7132b8034faab2c4ca0f1457817e",
            "bd43c0b7380848ca895fe3549f5cd026",
            "72f068ea39f34f029befd61e6eed8a01",
            "80bce61590774183a958d9bef75e4570",
            "db465a1d947c4be8b644f42fbf2bed0d",
            "70afa97ef56343bd8bf626cada248f33",
            "72d474e6c59c4c3eb74472b6bade534a",
            "c04c72453dae4ac0a1dc620a302bba66",
            "0c23e4fc5b1f4f1a9d53065d89a70126",
            "3740b930a986432eb2bf268b3a1802b5",
            "9fa09fad529e4e7288dd6da39d44b251",
            "4a90c0b84d1a45fe8bf0b0b4a549cd4b",
            "0ae40bd0934742de80d3d0463f9323a6",
            "efb2eaea978a4163a0031a6ad22aff2d",
            "e5cd4d4e529640ddad1e8d4e03e1e8d0",
            "6cf01f160d9a4b488d6ae523286b9a80",
            "8d9ff4ab32ad455b8c1da6642b6766e4",
            "96252479d7c94deeb230fec25f93068f",
            "d732757ace084912a9a7eda78b729d7c",
            "2774164fcca8488cba8354a5383e09b8",
            "7e3e25d0a25148cebf4c4135a2c29810",
            "da69ef6419c549aa908fcd5b5a161cd5",
            "a6ed2c195ae84ff0b4c9ffe0884eed20",
            "11d703e72c0d44968bfad1f5e2b51506",
            "5333241cf6dc496da8104f59eff4cad0",
            "5e5c5124f0854c26aaf0fd53a6c7149b",
            "07f0ace4af3e49089525587004830285",
            "dc93d97276fa4727a664bb61371e1db8",
            "6bcffff8f62248359d6660270054c575",
            "6336fb10bf4f4e2690f179db26ef9efa",
            "afc581deb5404f968f42e2278a2b1175"
          ]
        },
        "outputId": "5a196c2b-78d6-44ff-dfc7-16557d150dfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset Statistics:\n",
            "Total conversations: 600\n",
            "\n",
            "Category distribution:\n",
            "category\n",
            "git         212\n",
            "github      169\n",
            "greeting    117\n",
            "error       102\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Intent distribution:\n",
            "intent\n",
            "pull_request      169\n",
            "branch            125\n",
            "welcome           117\n",
            "merge_conflict    102\n",
            "commit             87\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "<ipython-input-34-0b38cde2a753>:34: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1:   0%|          | 0/135 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2805b84de7da4768aec4a456a972c32a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-0b38cde2a753>:84: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 2:   0%|          | 0/135 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3740b930a986432eb2bf268b3a1802b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 3:   0%|          | 0/135 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e3e25d0a25148cebf4c4135a2c29810"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tzip warning: name not matched: /content/drive/My\n",
            "\tzip warning: name not matched: Drive/chatbot\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r /content/chatbot_model.zip . -i /content/drive/My Drive/chatbot)\n",
            "\n",
            "Training complete! Model saved and ready for Streamlit deployment.\n",
            "Download the chatbot_model.zip file and extract it to your Streamlit project directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Testing Script\n",
        "import torch\n",
        "from typing import Dict, List, Tuple\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "class ChatbotTester:\n",
        "    def __init__(self, bot, test_data=None):\n",
        "        \"\"\"Initialize the tester with a trained bot and optional test data.\"\"\"\n",
        "        self.bot = bot\n",
        "        self.test_data = test_data\n",
        "        self.conversation_history = []\n",
        "        self.metrics = {\n",
        "            'response_times': [],\n",
        "            'confidence_scores': [],\n",
        "            'category_accuracy': [],\n",
        "            'intent_accuracy': []\n",
        "        }\n",
        "\n",
        "    def generate_response(self, user_input: str) -> Tuple[str, Dict]:\n",
        "        \"\"\"Generate a response and capture metrics.\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Generate response with metadata\n",
        "        response = self.bot.generate_response(user_input)\n",
        "\n",
        "        # Calculate response time\n",
        "        response_time = time.time() - start_time\n",
        "\n",
        "        # Store interaction\n",
        "        self.conversation_history.append({\n",
        "            'user_input': user_input,\n",
        "            'bot_response': response['text'],\n",
        "            'confidence': response.get('confidence', 0.0),\n",
        "            'category': response.get('category', ''),\n",
        "            'intent': response.get('intent', ''),\n",
        "            'response_time': response_time\n",
        "        })\n",
        "\n",
        "        return response['text'], response\n",
        "\n",
        "    def interactive_session(self):\n",
        "        \"\"\"Start an interactive testing session.\"\"\"\n",
        "        print(\"\\nStarting interactive testing session (type 'exit' to end)...\")\n",
        "        print(\"--------------------------------------------------------\")\n",
        "\n",
        "        while True:\n",
        "            user_input = input(\"\\nYou: \").strip()\n",
        "\n",
        "            if user_input.lower() == 'exit':\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                response, metadata = self.generate_response(user_input)\n",
        "                print(f\"\\nBot: {response}\")\n",
        "\n",
        "                # Print detailed metrics if available\n",
        "                if metadata.get('confidence'):\n",
        "                    print(f\"Confidence: {metadata['confidence']:.2f}\")\n",
        "                if metadata.get('category'):\n",
        "                    print(f\"Detected Category: {metadata['category']}\")\n",
        "                if metadata.get('intent'):\n",
        "                    print(f\"Detected Intent: {metadata['intent']}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating response: {str(e)}\")\n",
        "\n",
        "    def run_automated_tests(self, test_cases: List[Dict] = None):\n",
        "        \"\"\"Run automated tests with provided test cases or test data.\"\"\"\n",
        "        if test_cases is None and self.test_data is not None:\n",
        "            test_cases = self.test_data.to_dict('records')\n",
        "\n",
        "        if not test_cases:\n",
        "            raise ValueError(\"No test cases provided!\")\n",
        "\n",
        "        print(\"\\nRunning automated tests...\")\n",
        "        results = []\n",
        "\n",
        "        for case in tqdm(test_cases):\n",
        "            try:\n",
        "                response, metadata = self.generate_response(case['input'])\n",
        "\n",
        "                # Compare with expected outputs if provided\n",
        "                result = {\n",
        "                    'input': case['input'],\n",
        "                    'response': response,\n",
        "                    'expected_category': case.get('category'),\n",
        "                    'predicted_category': metadata.get('category'),\n",
        "                    'expected_intent': case.get('intent'),\n",
        "                    'predicted_intent': metadata.get('intent'),\n",
        "                    'confidence': metadata.get('confidence', 0.0),\n",
        "                    'response_time': self.conversation_history[-1]['response_time']\n",
        "                }\n",
        "\n",
        "                results.append(result)\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error testing case {case}: {str(e)}\")\n",
        "\n",
        "        return self._analyze_results(results)\n",
        "\n",
        "    def _analyze_results(self, results: List[Dict]) -> Dict:\n",
        "        \"\"\"Analyze test results and compute metrics.\"\"\"\n",
        "        metrics = {\n",
        "            'total_tests': len(results),\n",
        "            'successful_tests': sum(1 for r in results if r['confidence'] > 0.5),\n",
        "            'avg_confidence': np.mean([r['confidence'] for r in results]),\n",
        "            'avg_response_time': np.mean([r['response_time'] for r in results]),\n",
        "            'category_accuracy': None,\n",
        "            'intent_accuracy': None\n",
        "        }\n",
        "\n",
        "        # Calculate accuracies if expected values were provided\n",
        "        if all('expected_category' in r for r in results):\n",
        "            category_matches = sum(1 for r in results\n",
        "                                 if r['expected_category'] == r['predicted_category'])\n",
        "            metrics['category_accuracy'] = category_matches / len(results)\n",
        "\n",
        "        if all('expected_intent' in r for r in results):\n",
        "            intent_matches = sum(1 for r in results\n",
        "                               if r['expected_intent'] == r['predicted_intent'])\n",
        "            metrics['intent_accuracy'] = intent_matches / len(results)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the trained bot\n",
        "    try:\n",
        "        bot = MemoryOptimizedChatBot()\n",
        "        bot.load_model(os.path.join(SAVE_DIR, 'final_model'))\n",
        "\n",
        "        # Initialize tester\n",
        "        tester = ChatbotTester(bot)\n",
        "\n",
        "        # Run interactive testing session\n",
        "        print(\"\\nStarting interactive testing...\")\n",
        "        tester.interactive_session()\n",
        "\n",
        "        # Optional: Run automated tests if you have test cases\n",
        "        test_cases = [\n",
        "            {\n",
        "                'input': 'What are your store hours?',\n",
        "                'category': 'general_inquiry',\n",
        "                'intent': 'hours_inquiry'\n",
        "            },\n",
        "            {\n",
        "                'input': 'I need to return a product',\n",
        "                'category': 'customer_service',\n",
        "                'intent': 'return_request'\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        print(\"\\nRunning automated tests...\")\n",
        "        metrics = tester.run_automated_tests(test_cases)\n",
        "\n",
        "        print(\"\\nTest Results:\")\n",
        "        print(\"-------------\")\n",
        "        for metric, value in metrics.items():\n",
        "            if isinstance(value, float):\n",
        "                print(f\"{metric}: {value:.2f}\")\n",
        "            else:\n",
        "                print(f\"{metric}: {value}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in testing: {str(e)}\")\n",
        "        raise e"
      ],
      "metadata": {
        "id": "_PcrCPD-xr71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Performance Visualization\n",
        "class PerformanceVisualizer:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_dir = save_dir\n",
        "        self.metrics = {\n",
        "            'train_loss': [],\n",
        "            'val_loss': [],\n",
        "            'accuracy': [],\n",
        "            'epochs': []\n",
        "        }\n",
        "\n",
        "    def log_metrics(self, epoch, train_loss, val_loss, accuracy):\n",
        "        self.metrics['train_loss'].append(train_loss)\n",
        "        self.metrics['val_loss'].append(val_loss)\n",
        "        self.metrics['accuracy'].append(accuracy)\n",
        "        self.metrics['epochs'].append(epoch)\n",
        "\n",
        "    def plot_training_curves(self):\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Loss curves\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.metrics['epochs'], self.metrics['train_loss'], label='Training Loss')\n",
        "        plt.plot(self.metrics['epochs'], self.metrics['val_loss'], label='Validation Loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Accuracy curve\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.metrics['epochs'], self.metrics['accuracy'], label='Accuracy')\n",
        "        plt.title('Model Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(self.save_dir, 'training_curves.png'))\n",
        "        plt.close()\n",
        "\n",
        "    def save_metrics(self):\n",
        "        with open(os.path.join(self.save_dir, 'training_metrics.json'), 'w') as f:\n",
        "            json.dump(self.metrics, f, indent=2)\n"
      ],
      "metadata": {
        "id": "GtGHyhTNi8W-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Model Evaluation\n",
        "class ModelEvaluator:\n",
        "    def __init__(self, model, tokenizer, device):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = device\n",
        "\n",
        "    def evaluate_response(self, generated, reference):\n",
        "        \"\"\"Calculate BLEU score and response similarity\"\"\"\n",
        "        from nltk.translate.bleu_score import sentence_bleu\n",
        "        from nltk.tokenize import word_tokenize\n",
        "        import nltk\n",
        "        nltk.download('punkt')\n",
        "\n",
        "        # Tokenize sentences\n",
        "        reference_tokens = word_tokenize(reference.lower())\n",
        "        generated_tokens = word_tokenize(generated.lower())\n",
        "\n",
        "        # Calculate BLEU score\n",
        "        bleu_score = sentence_bleu([reference_tokens], generated_tokens)\n",
        "\n",
        "        # Calculate word overlap\n",
        "        ref_set = set(reference_tokens)\n",
        "        gen_set = set(generated_tokens)\n",
        "        overlap = len(ref_set.intersection(gen_set)) / len(ref_set.union(gen_set))\n",
        "\n",
        "        return {\n",
        "            'bleu_score': bleu_score,\n",
        "            'overlap_score': overlap\n",
        "        }\n",
        "\n",
        "    def evaluate_sample(self, category, intent, question, reference):\n",
        "        \"\"\"Evaluate a single sample\"\"\"\n",
        "        generated = self.model.generate_response(category, intent, question)\n",
        "        metrics = self.evaluate_response(generated, reference)\n",
        "        return {\n",
        "            'generated': generated,\n",
        "            'reference': reference,\n",
        "            'metrics': metrics\n",
        "        }\n",
        "\n",
        "    def run_evaluation(self, test_data, num_samples=100):\n",
        "        \"\"\"Run full evaluation on test set\"\"\"\n",
        "        results = []\n",
        "        total_bleu = 0\n",
        "        total_overlap = 0\n",
        "\n",
        "        samples = test_data.sample(n=min(num_samples, len(test_data)))\n",
        "\n",
        "        for _, row in tqdm(samples.iterrows(), total=len(samples), desc=\"Evaluating\"):\n",
        "            eval_result = self.evaluate_sample(\n",
        "                row['category'],\n",
        "                row['intent'],\n",
        "                row['user_input'],\n",
        "                row['bot_response']\n",
        "            )\n",
        "            results.append({\n",
        "                'category': row['category'],\n",
        "                'intent': row['intent'],\n",
        "                'question': row['user_input'],\n",
        "                'generated': eval_result['generated'],\n",
        "                'reference': eval_result['reference'],\n",
        "                'bleu_score': eval_result['metrics']['bleu_score'],\n",
        "                'overlap_score': eval_result['metrics']['overlap_score']\n",
        "            })\n",
        "\n",
        "            total_bleu += eval_result['metrics']['bleu_score']\n",
        "            total_overlap += eval_result['metrics']['overlap_score']\n",
        "\n",
        "        avg_metrics = {\n",
        "            'average_bleu': total_bleu / len(results),\n",
        "            'average_overlap': total_overlap / len(results)\n",
        "        }\n",
        "\n",
        "        return results, avg_metrics"
      ],
      "metadata": {
        "id": "vC6GHCi9i-uH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}